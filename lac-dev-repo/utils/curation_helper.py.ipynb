{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32ad3928-4042-40e6-9ba3-34e9e86c070d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "# Import libraries and functions\n",
    "from pyspark.sql import SparkSession\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# import logging\n",
    "import logging\n",
    "\n",
    "# Set up logging config\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "log = logging.getLogger(\"bronze_to_silver_logger\")\n",
    "from datetime import datetime\n",
    "\n",
    "from pyspark.sql.functions import (\n",
    "    trim,\n",
    "    lower,\n",
    "    upper,\n",
    "    date_format,\n",
    "    col\n",
    ")\n",
    "\n",
    "\n",
    "# Initialize log\n",
    "results_log = []\n",
    "\n",
    "def get_spark():\n",
    "    return SparkSession.builder.getOrCreate()\n",
    "\n",
    "# For authentication\n",
    "def fn_adls_connector():\n",
    "  try:\n",
    "      spark = get_spark()\n",
    "      spark.conf.set(\"fs.azure.account.key.storagec.dfs.core.windows.net\",\n",
    "                     \"====\")\n",
    "      print ('Connection to datalake established')\n",
    "\n",
    "  except Exception as ex:\n",
    "      print ('Connection to datalake failed')\n",
    "      print (ex)\n",
    "\n",
    "# For getting the list of tables in entity\n",
    "def fn_get_entity_list(entity_name: str, is_all_entity: str):\n",
    "    spark = get_spark()\n",
    "\n",
    "    entity_name = entity_name or \"\" \n",
    "    is_all_entity = str(is_all_entity).strip().lower() == \"yes\"\n",
    "    if not is_all_entity and not entity_name.strip():\n",
    "        print(\"Error: Missing parameter: Entity name cannot be empty when is_all_entity is not 'yes'\")\n",
    "        results_log.append({\n",
    "                    \"ExecutionDate\": datetime.now(),\n",
    "                    \"LogDate\": datetime.now().date(),\n",
    "                    \"Stage\": \"bronze_to_silver\",\n",
    "                    \"EntityName\": \"\",\n",
    "                    \"Operation\": \"Parameter Validation\",\n",
    "                    \"Status\": \"Failed\",\n",
    "                    \"ErrorMessage\": \"Missing Parameter\"\n",
    "                })\n",
    "        results_df = spark.createDataFrame(results_log)\n",
    "        results_df.write.format(\"delta\").mode(\"append\").saveAsTable(\"ceo_brz_dev.dbo.entity_log\")\n",
    "        raise ValueError(\"Entity name cannot be empty when is_all_entity is not 'yes'\")\n",
    "    \n",
    "    entity_list = [f\"'{name.strip().lower().strip('\"')}'\" for name in entity_name.split(',')]\n",
    "\n",
    "    entity_list_str = ','.join(entity_list)\n",
    "\n",
    "    if is_all_entity:\n",
    "        where_clause = \"WHERE IsFileAvailableInBronze = TRUE\"\n",
    "    else:\n",
    "        where_clause = f\"WHERE IsFileAvailableInBronze = TRUE AND LOWER(e.EntityName) IN ({entity_list_str})\"\n",
    "\n",
    "    entity_df = spark.sql(f\"\"\"\n",
    "                          SELECT * FROM `ceo_brz_dev`.`dbo`.`entity` e\n",
    "                          {where_clause}\n",
    "                          \"\"\")\n",
    "    return entity_df\n",
    "\n",
    "\n",
    "def fn_register_uc_table (table_format, table_catalog, table_schema, table_name, table_path):\n",
    "    spark = get_spark()\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS {table_catalog}.{table_schema}.{table_name}\n",
    "        USING {table_format}\n",
    "        LOCATION '{table_path}'\n",
    "    \"\"\")\n",
    "\n",
    "def fn_write_to_analytics_table(df, table_name):\n",
    "    df.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").saveAsTable(f\"ceo_analytics_dev.dbo.{table_name}\")\n",
    "\n",
    "# For applying data cleansing\n",
    "\n",
    "def fn_apply_cleansing (df, schema_df):\n",
    "    schema = schema_df.collect()\n",
    "    primary_key = next((row[\"SilverColumnName\"] for row in schema if str(row[\"IsPrimaryKey\"]).lower()==\"true\"), None)\n",
    "    for row in schema:\n",
    "        source_col = row[\"BronzeColumnName\"]\n",
    "        target_col = row[\"SilverColumnName\"]\n",
    "        cleansing_type = row[\"CleansingRule\"]\n",
    "        data_type = row[\"SilverDataType\"]\n",
    "\n",
    "        expr = col(source_col)  \n",
    "    \n",
    "        # if multiple cleansing rules per column\n",
    "        if cleansing_type:\n",
    "            rules = [r.strip().lower() for r in cleansing_type.split(\",\")]\n",
    "\n",
    "            for rule in rules:\n",
    "                if 'trim' in rule:\n",
    "                    expr = trim(expr)\n",
    "                elif 'lowercase' in rule:\n",
    "                    expr = lower(expr)\n",
    "                elif 'uppercase' in rule:\n",
    "                    expr = upper(expr)\n",
    "                elif 'to_date' in rule:\n",
    "                    expr = date_format(expr, 'yyyy-MM-dd')\n",
    "                elif 'to_timestamp' in rule:\n",
    "                    expr = date_format(expr, 'yyyy-MM-dd HH:mm:ss')\n",
    "\n",
    "        # cast to target data type and set silver column name       \n",
    "        expr = expr.cast(data_type).alias(target_col)\n",
    "        df = df.withColumn(target_col, expr)\n",
    "\n",
    "        # de-duplicate the data\n",
    "        if primary_key:\n",
    "            df = df.dropDuplicates([primary_key])\n",
    "\n",
    "    target_cols = [row[\"SilverColumnName\"] for row in schema]\n",
    "    df = df.select(*target_cols)\n",
    "    return df,primary_key\n",
    "\n",
    "def fn_get_entity_schema(table_name):\n",
    "    spark = get_spark()\n",
    "    return spark.sql(f\"\"\"\n",
    "        SELECT      s.* \n",
    "        FROM        `ceo_brz_dev`.`dbo`.`entity_schema` s \n",
    "        INNER JOIN  `ceo_brz_dev`.`dbo`.`entity` e ON\n",
    "                    e.EntityId = s.EntityId \n",
    "        WHERE e.EntityName = '{table_name}'\n",
    "        \"\"\")\n",
    "\n",
    "def fn_read_bronze(files_path):\n",
    "    spark = get_spark()\n",
    "    return spark.read.option(\"recursiveFileLookup\", \"true\").parquet(files_path)\n",
    "\n",
    "\n",
    "def fn_write_to_silver(cleaned_bronze_df,load_type,silver_path,primary_key,entity_name,table_format,table_catalog,table_schema,silver_name):\n",
    "    spark = get_spark()\n",
    "    if DeltaTable.isDeltaTable(spark, silver_path):\n",
    "        if load_type == \"Incremental\":\n",
    "            silver_table = DeltaTable.forPath(spark, silver_path)\n",
    "            merge_condition = f\"target.{primary_key} = source.{primary_key}\"\n",
    "            (\n",
    "                silver_table.alias(\"target\")\n",
    "                .merge(\n",
    "                    cleaned_bronze_df.alias(\"source\"),\n",
    "                    merge_condition\n",
    "                    )\n",
    "                .whenMatchedUpdateAll()\n",
    "                .whenNotMatchedInsertAll()\n",
    "                .execute()\n",
    "                )\n",
    "            print(f\"Merging/Incremental delta: {entity_name}\")\n",
    "            return \"Incremental Merge\"\n",
    "        else:\n",
    "            cleaned_bronze_df.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").save(silver_path)\n",
    "            print(f\"Full Load delta: {entity_name}\")\n",
    "            return \"Full Load Overwrite\"\n",
    "            \n",
    "    else:\n",
    "        cleaned_bronze_df.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"true\").save(silver_path)\n",
    "        fn_register_uc_table(\n",
    "            table_format=table_format, \n",
    "            table_catalog=table_catalog, \n",
    "            table_schema=table_schema,\n",
    "            table_name=silver_name, \n",
    "            table_path=silver_path)\n",
    "        print(f\"Creating delta: {entity_name}\")\n",
    "        return \"Delta Table Created\"\n",
    "\n",
    "def fn_update_entity_metadata(entity_name):\n",
    "    spark = get_spark()\n",
    "    spark.sql(f\"\"\"\n",
    "        UPDATE ceo_brz_dev.dbo.entity\n",
    "        SET LastExtractionDate = current_timestamp(),\n",
    "            LoadType = 'Incremental'\n",
    "        WHERE EntityName = '{entity_name}'\n",
    "        \"\"\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "sql",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "curation_helper.py",
   "widgets": {}
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
